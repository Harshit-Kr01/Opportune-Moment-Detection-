{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c505c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Activation, Dropout, Bidirectional, Multiply, Flatten, Lambda,RepeatVector, Permute \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from scipy.signal import resample, butter, filtfilt, welch\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344d4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for sampling rates\n",
    "PHYSIO_SAMPLING_RATE = 1000  # Hz\n",
    "VALENCE_SAMPLING_RATE = 20    # Hz\n",
    "DOWNSAMPLE_FACTOR = PHYSIO_SAMPLING_RATE // VALENCE_SAMPLING_RATE  # 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4ba599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    columns = [\"time\", \"ECG\", \"BVP\", \"GSR\", \"Resp\", \"Skin_Temp\", \"EMG_z\", \"EMG_c\", \"EMG_t\"]\n",
    "    \n",
    "    # Read all data first\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", names=columns)\n",
    "    \n",
    "    # Downsample physiological data to match valence-arousal sampling rate\n",
    "    downsampled_df = df.iloc[::DOWNSAMPLE_FACTOR, :].copy()\n",
    "    \n",
    "    # Reset time to new sampling rate\n",
    "    downsampled_df[\"time\"] = np.arange(len(downsampled_df)) / VALENCE_SAMPLING_RATE\n",
    "    \n",
    "    return downsampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d417da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment into 5-second windows with proper alignment\n",
    "def segment_data(physio_df, window_size=5):  # Removed valence_df parameter\n",
    "    # Calculate number of samples per window\n",
    "    samples_per_window = window_size * VALENCE_SAMPLING_RATE\n",
    "    \n",
    "    # Segment physiological data only\n",
    "    segments = []\n",
    "    for i in range(0, len(physio_df), samples_per_window):\n",
    "        segment = physio_df.iloc[i:i+samples_per_window]\n",
    "        if len(segment) == samples_per_window:  # only complete segments\n",
    "            # Calculate time-domain features\n",
    "            features = {\n",
    "                \"time\": segment[\"time\"].mean(),\n",
    "                \"ECG_mean\": segment[\"ECG\"].mean(),\n",
    "                \"ECG_std\": segment[\"ECG\"].std(),\n",
    "                \"ECG_hr\": 60 / (segment[\"ECG\"].diff().abs().mean() + 1e-6),\n",
    "                \"BVP_mean\": segment[\"BVP\"].mean(),\n",
    "                \"BVP_std\": segment[\"BVP\"].std(),\n",
    "                \"GSR_mean\": segment[\"GSR\"].mean(),\n",
    "                \"GSR_std\": segment[\"GSR\"].std(),\n",
    "                \"GSR_slope\": np.polyfit(np.arange(len(segment)), segment[\"GSR\"], 1)[0],\n",
    "                \"Resp_mean\": segment[\"Resp\"].mean(),\n",
    "                \"Resp_std\": segment[\"Resp\"].std(),\n",
    "                \"Resp_rate\": len(np.where(np.diff(np.sign(segment[\"Resp\"] - segment[\"Resp\"].mean())))[0]) / 2,\n",
    "                \"Skin_temp_mean\": segment[\"Skin_Temp\"].mean(),\n",
    "                \"Skin_temp_std\": segment[\"Skin_Temp\"].std(),\n",
    "                \"EMG_mean\": segment[[\"EMG_z\", \"EMG_c\", \"EMG_t\"]].mean().mean(),\n",
    "                \"EMG_std\": segment[[\"EMG_z\", \"EMG_c\", \"EMG_t\"]].std().mean()\n",
    "            }\n",
    "            segments.append(features)\n",
    "    \n",
    "    return pd.DataFrame(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f6c6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_physiological_features(segment):\n",
    "    \"\"\"Extract time and frequency domain features from a physiological segment\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Time-domain features\n",
    "    for signal in ['ECG', 'BVP', 'GSR', 'Resp', 'Skin_Temp']:\n",
    "        sig_data = segment[signal].values\n",
    "        features.update({\n",
    "            f\"{signal}_mean\": np.mean(sig_data),\n",
    "            f\"{signal}_std\": np.std(sig_data),\n",
    "            f\"{signal}_max\": np.max(sig_data),\n",
    "            f\"{signal}_min\": np.min(sig_data),\n",
    "            f\"{signal}_range\": np.ptp(sig_data),\n",
    "            f\"{signal}_slope\": np.polyfit(np.arange(len(sig_data)), sig_data, 1)[0],\n",
    "            f\"{signal}_diff_mean\": np.mean(np.diff(sig_data)),\n",
    "            f\"{signal}_diff_std\": np.std(np.diff(sig_data))\n",
    "        })\n",
    "    \n",
    "    # Frequency-domain features (using Welch's method)\n",
    "    for signal in ['ECG', 'BVP', 'GSR']:\n",
    "        sig_data = segment[signal].values\n",
    "        f, Pxx = welch(sig_data, fs=VALENCE_SAMPLING_RATE, nperseg=min(len(sig_data), 256))\n",
    "        features.update({\n",
    "            f\"{signal}_psd_mean\": np.mean(Pxx),\n",
    "            f\"{signal}_psd_std\": np.std(Pxx),\n",
    "            f\"{signal}_psd_max\": np.max(Pxx),\n",
    "            f\"{signal}_psd_max_freq\": f[np.argmax(Pxx)],\n",
    "            f\"{signal}_psd_ratio\": np.sum(Pxx[f > 0.1]) / (np.sum(Pxx) + 1e-6)  # ratio of high freq components\n",
    "        })\n",
    "    \n",
    "    # Combined features\n",
    "    features['HRV'] = np.std(np.diff(np.where(np.diff(segment['ECG']) > 0.5 * np.std(segment['ECG']))[0]))\n",
    "    features['GSR_peaks'] = len(find_peaks(segment['GSR'], height=np.mean(segment['GSR']))[0])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc186654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rulsif_change_scores(X, alpha=0.1, sigma=1.0, lambda_param=1e-3):\n",
    "    n = len(X) - 1\n",
    "    change_scores = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        X_t, X_t_next = X[i], X[i + 1]\n",
    "        \n",
    "        # Compute Gaussian Kernel Matrix\n",
    "        K_t = rbf_kernel(X_t.reshape(-1, 1), X_t.reshape(-1, 1), gamma=1/(2*sigma**2))\n",
    "        K_t_next = rbf_kernel(X_t_next.reshape(-1, 1), X_t_next.reshape(-1, 1), gamma=1/(2*sigma**2))\n",
    "        \n",
    "        # Compute Weights using Least Squares Importance Fitting (LSIF)\n",
    "        H = alpha * K_t + (1 - alpha) * K_t_next + lambda_param * np.eye(K_t.shape[0])\n",
    "        h = np.mean(K_t, axis=1)\n",
    "        \n",
    "        theta = np.linalg.solve(H, h)\n",
    "        \n",
    "        # Compute Change Score\n",
    "        change_scores[i] = np.mean(np.square(K_t_next.dot(theta) - 1))\n",
    "    \n",
    "    return change_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1ab938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_opportune_moments(change_scores):\n",
    "    mean, std = np.mean(change_scores), np.std(change_scores)\n",
    "    threshold = mean + 3 * std\n",
    "    outliers = change_scores > threshold\n",
    "    \n",
    "    # Clustering the remaining scores\n",
    "    valid_indices = np.where(~outliers)[0]  # Indices of non-outliers\n",
    "    valid_scores = change_scores[valid_indices]\n",
    "    \n",
    "    if len(valid_scores) > 1:  # Ensure there are enough samples for clustering\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42).fit(valid_scores.reshape(-1, 1))\n",
    "        centroids = kmeans.cluster_centers_.flatten()\n",
    "        high_cluster = np.argmax(centroids)\n",
    "        high_values = (kmeans.labels_ == high_cluster) & (valid_scores > centroids[high_cluster])\n",
    "        \n",
    "        # Map high_values back to the original indices\n",
    "        high_values_mapped = np.zeros_like(change_scores, dtype=bool)\n",
    "        high_values_mapped[valid_indices] = high_values\n",
    "    else:\n",
    "        high_values_mapped = np.zeros_like(change_scores, dtype=bool)\n",
    "    \n",
    "    # Mark opportune moments\n",
    "    opportune_moments = np.where(outliers | high_values_mapped)[0]\n",
    "    return opportune_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a9966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(segmented_data, change_scores):\n",
    "    # Select only physiological features\n",
    "    features = [\n",
    "        'ECG_mean', 'ECG_std', 'ECG_hr', \n",
    "        'BVP_mean', 'BVP_std',\n",
    "        'GSR_mean', 'GSR_std', 'GSR_slope',\n",
    "        'Resp_mean', 'Resp_std', 'Resp_rate',\n",
    "        'Skin_temp_mean', 'Skin_temp_std',\n",
    "        'EMG_mean', 'EMG_std'\n",
    "    ]\n",
    "    \n",
    "    # Add change point scores\n",
    "    segmented_data['change_score'] = np.concatenate([[0], change_scores])[:len(segmented_data)]\n",
    "    \n",
    "    # Prepare input sequence\n",
    "    input_sequence = segmented_data[features + ['change_score']].values\n",
    "    \n",
    "    # Robust scaling\n",
    "    scaler = RobustScaler()\n",
    "    input_sequence = scaler.fit_transform(input_sequence)\n",
    "    \n",
    "    return input_sequence\n",
    "\n",
    "def prepare_input_student(segmented_data, change_scores):\n",
    "    # Features only from BVP, GSR, and Skin Temperature + change scores\n",
    "    features = [\n",
    "        'BVP_mean', 'BVP_std',\n",
    "        'GSR_mean', 'GSR_std', 'GSR_slope',\n",
    "        'Skin_temp_mean', 'Skin_temp_std'\n",
    "    ]\n",
    "    \n",
    "    # Add change point scores (same as before)\n",
    "    segmented_data['change_score'] = np.concatenate([[0], change_scores])[:len(segmented_data)]\n",
    "    \n",
    "    # Prepare input sequence with selected features\n",
    "    input_sequence = segmented_data[features + ['change_score']].values\n",
    "    \n",
    "    # Apply robust scaling\n",
    "    scaler = RobustScaler()\n",
    "    input_sequence = scaler.fit_transform(input_sequence)\n",
    "    \n",
    "    return input_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d77861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elliott(x, p):\n",
    "    return (p * x) / (1 + K.abs(x))\n",
    "\n",
    "# Derivative of PEF\n",
    "def elliott_derivative(x, p):\n",
    "    return p / ((K.abs(x) + 1) ** 2)\n",
    "\n",
    "# Swish Activation Function\n",
    "def swish(x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "# Custom PEF Activation Layer\n",
    "class PEFLayer(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(PEFLayer, self).__init__(activation, **kwargs)\n",
    "        self.p = K.variable(1.0)  # Initialize parameter p\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return elliott(inputs, self.p)\n",
    "\n",
    "# def build_p_lstm(input_shape):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "    \n",
    "#     # p-LSTM Layer with PEF activation\n",
    "#     lstm_out = LSTM(32, return_sequences=False, kernel_regularizer=l2(0.01))(inputs)\n",
    "#     lstm_out = Dropout(0.5)(lstm_out)  # Dropout for regularization\n",
    "#     lstm_out = PEFLayer(elliott)(lstm_out)\n",
    "    \n",
    "#     # Fully Connected Layers with PEF activation\n",
    "#     dense1 = Dense(16, kernel_regularizer=l2(0.01))(lstm_out)\n",
    "#     dense1 = Dropout(0.5)(dense1)  # Dropout for regularization\n",
    "#     dense1 = PEFLayer(elliott)(dense1)\n",
    "    \n",
    "#     # Swish Activation Layer\n",
    "#     swish_out = Activation(swish)(dense1)\n",
    "    \n",
    "#     # Sigmoid Output Layer for Binary Classification\n",
    "#     outputs = Dense(1, activation=\"sigmoid\")(swish_out)\n",
    "    \n",
    "#     # Define Model\n",
    "#     model = Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "def build_p_lstm_for_distillation(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # p-LSTM Layer with PEF activation (named)\n",
    "    lstm_out = LSTM(32, return_sequences=False, kernel_regularizer=l2(0.01), name='lstm_layer')(inputs)\n",
    "    lstm_out = Dropout(0.5)(lstm_out)  # Dropout for regularization\n",
    "    lstm_out = PEFLayer(elliott)(lstm_out)\n",
    "    \n",
    "    # Fully Connected Layer with PEF activation (named)\n",
    "    dense1 = Dense(16, kernel_regularizer=l2(0.01), name='dense_layer')(lstm_out)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense1 = PEFLayer(elliott)(dense1)\n",
    "    \n",
    "    # Swish Activation Layer\n",
    "    swish_out = Activation(swish)(dense1)\n",
    "    \n",
    "    # Sigmoid Output Layer for Binary Classification\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name='output_layer')(swish_out)\n",
    "    \n",
    "    # Model outputs intermediate layers + final output\n",
    "    model = Model(inputs=inputs, outputs=[lstm_out, dense1, outputs])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_student_lstm(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Smaller LSTM layer (named)\n",
    "    lstm_out = LSTM(16, return_sequences=False, kernel_regularizer=l2(0.01), name='lstm_layer')(inputs)\n",
    "    lstm_out = Dropout(0.5)(lstm_out)\n",
    "    lstm_out = PEFLayer(elliott)(lstm_out)\n",
    "    \n",
    "    # Smaller Dense layer (named)\n",
    "    dense1 = Dense(8, kernel_regularizer=l2(0.01), name='dense_layer')(lstm_out)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense1 = PEFLayer(elliott)(dense1)\n",
    "    \n",
    "    # Swish Activation\n",
    "    swish_out = Activation(swish)(dense1)\n",
    "    \n",
    "    # Sigmoid output for binary classification\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name='output_layer')(swish_out)\n",
    "    \n",
    "    # Multi-output: intermediate features + final output\n",
    "    model = Model(inputs=inputs, outputs=[lstm_out, dense1, outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8520493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harshit Kumar\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training User1 on: ['User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User1...\n",
      "Teacher Evaluation for User1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.95      0.94       341\n",
      "         1.0       0.88      0.82      0.85       149\n",
      "\n",
      "    accuracy                           0.91       490\n",
      "   macro avg       0.90      0.89      0.89       490\n",
      "weighted avg       0.91      0.91      0.91       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.964\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User1...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6482 | Val Loss: 0.4176\n",
      "Epoch 2/50 - Train Loss: 0.3971 | Val Loss: 0.2305\n",
      "Epoch 3/50 - Train Loss: 0.3050 | Val Loss: 0.2040\n",
      "Epoch 4/50 - Train Loss: 0.2947 | Val Loss: 0.1919\n",
      "Epoch 5/50 - Train Loss: 0.2820 | Val Loss: 0.1835\n",
      "Epoch 6/50 - Train Loss: 0.2723 | Val Loss: 0.1755\n",
      "Epoch 7/50 - Train Loss: 0.2679 | Val Loss: 0.1825\n",
      "Epoch 8/50 - Train Loss: 0.2649 | Val Loss: 0.1833\n",
      "Epoch 9/50 - Train Loss: 0.2611 | Val Loss: 0.1842\n",
      "Epoch 10/50 - Train Loss: 0.2561 | Val Loss: 0.1926\n",
      "Epoch 11/50 - Train Loss: 0.2559 | Val Loss: 0.1897\n",
      "Early stopping at epoch 11\n",
      "Evaluation for User1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      1.00      0.82       341\n",
      "         1.0       0.75      0.02      0.04       149\n",
      "\n",
      "    accuracy                           0.70       490\n",
      "   macro avg       0.72      0.51      0.43       490\n",
      "weighted avg       0.71      0.70      0.58       490\n",
      "\n",
      "AUC-ROC: 0.984\n",
      "\n",
      "Training User2 on: ['User1', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User2...\n",
      "Teacher Evaluation for User2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.80      0.89       430\n",
      "         1.0       0.41      0.98      0.58        60\n",
      "\n",
      "    accuracy                           0.82       490\n",
      "   macro avg       0.70      0.89      0.73       490\n",
      "weighted avg       0.92      0.82      0.85       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.986\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User2...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6185 | Val Loss: 0.3719\n",
      "Epoch 2/50 - Train Loss: 0.3877 | Val Loss: 0.1985\n",
      "Epoch 3/50 - Train Loss: 0.3208 | Val Loss: 0.1789\n",
      "Epoch 4/50 - Train Loss: 0.2985 | Val Loss: 0.1792\n",
      "Epoch 5/50 - Train Loss: 0.2846 | Val Loss: 0.1783\n",
      "Epoch 6/50 - Train Loss: 0.2778 | Val Loss: 0.1738\n",
      "Epoch 7/50 - Train Loss: 0.2725 | Val Loss: 0.1887\n",
      "Epoch 8/50 - Train Loss: 0.2642 | Val Loss: 0.1880\n",
      "Epoch 9/50 - Train Loss: 0.2622 | Val Loss: 0.1941\n",
      "Epoch 10/50 - Train Loss: 0.2645 | Val Loss: 0.1831\n",
      "Epoch 11/50 - Train Loss: 0.2616 | Val Loss: 0.1895\n",
      "Early stopping at epoch 11\n",
      "Evaluation for User2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.91      0.95       430\n",
      "         1.0       0.60      0.97      0.74        60\n",
      "\n",
      "    accuracy                           0.92       490\n",
      "   macro avg       0.80      0.94      0.84       490\n",
      "weighted avg       0.95      0.92      0.92       490\n",
      "\n",
      "AUC-ROC: 0.990\n",
      "\n",
      "Training User3 on: ['User1', 'User2', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User3...\n",
      "Teacher Evaluation for User3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.97       376\n",
      "         1.0       0.89      0.88      0.88       114\n",
      "\n",
      "    accuracy                           0.95       490\n",
      "   macro avg       0.93      0.92      0.93       490\n",
      "weighted avg       0.95      0.95      0.95       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.987\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User3...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6004 | Val Loss: 0.3373\n",
      "Epoch 2/50 - Train Loss: 0.3646 | Val Loss: 0.1961\n",
      "Epoch 3/50 - Train Loss: 0.3030 | Val Loss: 0.1805\n",
      "Epoch 4/50 - Train Loss: 0.2831 | Val Loss: 0.1836\n",
      "Epoch 5/50 - Train Loss: 0.2723 | Val Loss: 0.1902\n",
      "Epoch 6/50 - Train Loss: 0.2672 | Val Loss: 0.1922\n",
      "Epoch 7/50 - Train Loss: 0.2665 | Val Loss: 0.1880\n",
      "Epoch 8/50 - Train Loss: 0.2627 | Val Loss: 0.1903\n",
      "Early stopping at epoch 8\n",
      "Evaluation for User3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      1.00      0.91       376\n",
      "         1.0       0.98      0.37      0.54       114\n",
      "\n",
      "    accuracy                           0.85       490\n",
      "   macro avg       0.91      0.68      0.72       490\n",
      "weighted avg       0.87      0.85      0.82       490\n",
      "\n",
      "AUC-ROC: 0.987\n",
      "\n",
      "Training User4 on: ['User5', 'User18', 'User26', 'User29']\n",
      "Train samples for teacher: 1960, Test samples: 490\n",
      "\n",
      "Training teacher model for User4...\n",
      "Teacher Evaluation for User4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.89      0.94       433\n",
      "         1.0       0.53      0.95      0.68        57\n",
      "\n",
      "    accuracy                           0.90       490\n",
      "   macro avg       0.76      0.92      0.81       490\n",
      "weighted avg       0.94      0.90      0.91       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.964\n",
      "Train samples for student: 1960, Test samples: 490\n",
      "Training student model with distillation for User4...\n",
      "Teacher Train: 1568 | Teacher Val: 392 | Student Train: 1568 | Student Val: 392\n",
      "Epoch 1/50 - Train Loss: 0.7838 | Val Loss: 0.7013\n",
      "Epoch 2/50 - Train Loss: 0.7227 | Val Loss: 0.6277\n",
      "Epoch 3/50 - Train Loss: 0.6603 | Val Loss: 0.5424\n",
      "Epoch 4/50 - Train Loss: 0.5963 | Val Loss: 0.4520\n",
      "Epoch 5/50 - Train Loss: 0.5216 | Val Loss: 0.3722\n",
      "Epoch 6/50 - Train Loss: 0.4618 | Val Loss: 0.3223\n",
      "Epoch 7/50 - Train Loss: 0.4091 | Val Loss: 0.2752\n",
      "Epoch 8/50 - Train Loss: 0.3648 | Val Loss: 0.2457\n",
      "Epoch 9/50 - Train Loss: 0.3361 | Val Loss: 0.2407\n",
      "Epoch 10/50 - Train Loss: 0.3065 | Val Loss: 0.2324\n",
      "Epoch 11/50 - Train Loss: 0.2856 | Val Loss: 0.2290\n",
      "Epoch 12/50 - Train Loss: 0.2624 | Val Loss: 0.2373\n",
      "Epoch 13/50 - Train Loss: 0.2574 | Val Loss: 0.2339\n",
      "Epoch 14/50 - Train Loss: 0.2559 | Val Loss: 0.2332\n",
      "Epoch 15/50 - Train Loss: 0.2553 | Val Loss: 0.2512\n",
      "Epoch 16/50 - Train Loss: 0.2515 | Val Loss: 0.2702\n",
      "Early stopping at epoch 16\n",
      "Evaluation for User4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96       433\n",
      "         1.0       0.72      0.63      0.67        57\n",
      "\n",
      "    accuracy                           0.93       490\n",
      "   macro avg       0.84      0.80      0.82       490\n",
      "weighted avg       0.93      0.93      0.93       490\n",
      "\n",
      "AUC-ROC: 0.949\n",
      "\n",
      "Training User5 on: ['User4', 'User18', 'User26', 'User29']\n",
      "Train samples for teacher: 1960, Test samples: 490\n",
      "\n",
      "Training teacher model for User5...\n",
      "Teacher Evaluation for User5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.90      0.93       420\n",
      "         1.0       0.58      0.84      0.69        70\n",
      "\n",
      "    accuracy                           0.89       490\n",
      "   macro avg       0.78      0.87      0.81       490\n",
      "weighted avg       0.92      0.89      0.90       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.952\n",
      "Train samples for student: 1960, Test samples: 490\n",
      "Training student model with distillation for User5...\n",
      "Teacher Train: 1568 | Teacher Val: 392 | Student Train: 1568 | Student Val: 392\n",
      "Epoch 1/50 - Train Loss: 0.7676 | Val Loss: 0.7070\n",
      "Epoch 2/50 - Train Loss: 0.7118 | Val Loss: 0.6450\n",
      "Epoch 3/50 - Train Loss: 0.6557 | Val Loss: 0.5724\n",
      "Epoch 4/50 - Train Loss: 0.5932 | Val Loss: 0.4873\n",
      "Epoch 5/50 - Train Loss: 0.5232 | Val Loss: 0.4022\n",
      "Epoch 6/50 - Train Loss: 0.4651 | Val Loss: 0.3320\n",
      "Epoch 7/50 - Train Loss: 0.4131 | Val Loss: 0.2886\n",
      "Epoch 8/50 - Train Loss: 0.3656 | Val Loss: 0.2590\n",
      "Epoch 9/50 - Train Loss: 0.3283 | Val Loss: 0.2603\n",
      "Epoch 10/50 - Train Loss: 0.3004 | Val Loss: 0.2498\n",
      "Epoch 11/50 - Train Loss: 0.2855 | Val Loss: 0.2474\n",
      "Epoch 12/50 - Train Loss: 0.2804 | Val Loss: 0.2529\n",
      "Epoch 13/50 - Train Loss: 0.2619 | Val Loss: 0.2723\n",
      "Epoch 14/50 - Train Loss: 0.2556 | Val Loss: 0.2868\n",
      "Epoch 15/50 - Train Loss: 0.2364 | Val Loss: 0.2817\n",
      "Epoch 16/50 - Train Loss: 0.2236 | Val Loss: 0.3035\n",
      "Early stopping at epoch 16\n",
      "Evaluation for User5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.96       420\n",
      "         1.0       0.87      0.67      0.76        70\n",
      "\n",
      "    accuracy                           0.94       490\n",
      "   macro avg       0.91      0.83      0.86       490\n",
      "weighted avg       0.94      0.94      0.94       490\n",
      "\n",
      "AUC-ROC: 0.973\n",
      "\n",
      "Training User6 on: ['User1', 'User2', 'User3', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User6...\n",
      "Teacher Evaluation for User6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.76      0.86       429\n",
      "         1.0       0.37      0.98      0.54        61\n",
      "\n",
      "    accuracy                           0.79       490\n",
      "   macro avg       0.68      0.87      0.70       490\n",
      "weighted avg       0.92      0.79      0.82       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.972\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User6...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6210 | Val Loss: 0.3823\n",
      "Epoch 2/50 - Train Loss: 0.3917 | Val Loss: 0.2128\n",
      "Epoch 3/50 - Train Loss: 0.3176 | Val Loss: 0.1873\n",
      "Epoch 4/50 - Train Loss: 0.2944 | Val Loss: 0.1781\n",
      "Epoch 5/50 - Train Loss: 0.2855 | Val Loss: 0.1886\n",
      "Epoch 6/50 - Train Loss: 0.2706 | Val Loss: 0.1906\n",
      "Epoch 7/50 - Train Loss: 0.2706 | Val Loss: 0.1897\n",
      "Epoch 8/50 - Train Loss: 0.2585 | Val Loss: 0.1923\n",
      "Epoch 9/50 - Train Loss: 0.2599 | Val Loss: 0.1921\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.88      0.94       429\n",
      "         1.0       0.54      0.97      0.69        61\n",
      "\n",
      "    accuracy                           0.89       490\n",
      "   macro avg       0.77      0.93      0.81       490\n",
      "weighted avg       0.94      0.89      0.91       490\n",
      "\n",
      "AUC-ROC: 0.970\n",
      "\n",
      "Training User8 on: ['User1', 'User2', 'User3', 'User6', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User8...\n",
      "Teacher Evaluation for User8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.74      0.85       445\n",
      "         1.0       0.28      1.00      0.44        45\n",
      "\n",
      "    accuracy                           0.77       490\n",
      "   macro avg       0.64      0.87      0.65       490\n",
      "weighted avg       0.93      0.77      0.82       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.991\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User8...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6109 | Val Loss: 0.3486\n",
      "Epoch 2/50 - Train Loss: 0.3755 | Val Loss: 0.2022\n",
      "Epoch 3/50 - Train Loss: 0.3145 | Val Loss: 0.1870\n",
      "Epoch 4/50 - Train Loss: 0.2897 | Val Loss: 0.1736\n",
      "Epoch 5/50 - Train Loss: 0.2819 | Val Loss: 0.1877\n",
      "Epoch 6/50 - Train Loss: 0.2764 | Val Loss: 0.1822\n",
      "Epoch 7/50 - Train Loss: 0.2697 | Val Loss: 0.1862\n",
      "Epoch 8/50 - Train Loss: 0.2685 | Val Loss: 0.1903\n",
      "Epoch 9/50 - Train Loss: 0.2640 | Val Loss: 0.1920\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.93       445\n",
      "         1.0       0.42      1.00      0.60        45\n",
      "\n",
      "    accuracy                           0.88       490\n",
      "   macro avg       0.71      0.93      0.76       490\n",
      "weighted avg       0.95      0.88      0.90       490\n",
      "\n",
      "AUC-ROC: 0.985\n",
      "\n",
      "Training User9 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User9...\n",
      "Teacher Evaluation for User9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.72      0.84       447\n",
      "         1.0       0.25      0.98      0.40        43\n",
      "\n",
      "    accuracy                           0.75       490\n",
      "   macro avg       0.63      0.85      0.62       490\n",
      "weighted avg       0.93      0.75      0.80       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.970\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User9...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6166 | Val Loss: 0.3340\n",
      "Epoch 2/50 - Train Loss: 0.3720 | Val Loss: 0.1922\n",
      "Epoch 3/50 - Train Loss: 0.3153 | Val Loss: 0.1803\n",
      "Epoch 4/50 - Train Loss: 0.2912 | Val Loss: 0.1801\n",
      "Epoch 5/50 - Train Loss: 0.2860 | Val Loss: 0.1907\n",
      "Epoch 6/50 - Train Loss: 0.2770 | Val Loss: 0.2104\n",
      "Epoch 7/50 - Train Loss: 0.2716 | Val Loss: 0.1878\n",
      "Epoch 8/50 - Train Loss: 0.2737 | Val Loss: 0.2006\n",
      "Epoch 9/50 - Train Loss: 0.2656 | Val Loss: 0.1879\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94       447\n",
      "         1.0       0.45      0.98      0.62        43\n",
      "\n",
      "    accuracy                           0.89       490\n",
      "   macro avg       0.72      0.93      0.78       490\n",
      "weighted avg       0.95      0.89      0.91       490\n",
      "\n",
      "AUC-ROC: 0.973\n",
      "\n",
      "Training User10 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User10...\n",
      "Teacher Evaluation for User10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92       410\n",
      "         1.0       0.57      0.99      0.72        80\n",
      "\n",
      "    accuracy                           0.88       490\n",
      "   macro avg       0.78      0.92      0.82       490\n",
      "weighted avg       0.93      0.88      0.89       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.993\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User10...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6231 | Val Loss: 0.3492\n",
      "Epoch 2/50 - Train Loss: 0.3872 | Val Loss: 0.2020\n",
      "Epoch 3/50 - Train Loss: 0.3339 | Val Loss: 0.1755\n",
      "Epoch 4/50 - Train Loss: 0.3054 | Val Loss: 0.1740\n",
      "Epoch 5/50 - Train Loss: 0.2929 | Val Loss: 0.1779\n",
      "Epoch 6/50 - Train Loss: 0.2894 | Val Loss: 0.1805\n",
      "Epoch 7/50 - Train Loss: 0.2841 | Val Loss: 0.1862\n",
      "Epoch 8/50 - Train Loss: 0.2792 | Val Loss: 0.1900\n",
      "Epoch 9/50 - Train Loss: 0.2744 | Val Loss: 0.1914\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97       410\n",
      "         1.0       1.00      0.70      0.82        80\n",
      "\n",
      "    accuracy                           0.95       490\n",
      "   macro avg       0.97      0.85      0.90       490\n",
      "weighted avg       0.95      0.95      0.95       490\n",
      "\n",
      "AUC-ROC: 0.986\n",
      "\n",
      "Training User11 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User11...\n",
      "Teacher Evaluation for User11:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.71      0.83       417\n",
      "         1.0       0.37      0.99      0.54        73\n",
      "\n",
      "    accuracy                           0.75       490\n",
      "   macro avg       0.68      0.85      0.69       490\n",
      "weighted avg       0.90      0.75      0.79       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.974\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User11...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6141 | Val Loss: 0.3676\n",
      "Epoch 2/50 - Train Loss: 0.3961 | Val Loss: 0.2156\n",
      "Epoch 3/50 - Train Loss: 0.3328 | Val Loss: 0.1725\n",
      "Epoch 4/50 - Train Loss: 0.3127 | Val Loss: 0.1712\n",
      "Epoch 5/50 - Train Loss: 0.2979 | Val Loss: 0.1729\n",
      "Epoch 6/50 - Train Loss: 0.2844 | Val Loss: 0.1743\n",
      "Epoch 7/50 - Train Loss: 0.2825 | Val Loss: 0.1696\n",
      "Epoch 8/50 - Train Loss: 0.2775 | Val Loss: 0.1774\n",
      "Epoch 9/50 - Train Loss: 0.2732 | Val Loss: 0.1808\n",
      "Epoch 10/50 - Train Loss: 0.2695 | Val Loss: 0.1849\n",
      "Epoch 11/50 - Train Loss: 0.2644 | Val Loss: 0.1879\n",
      "Epoch 12/50 - Train Loss: 0.2642 | Val Loss: 0.1861\n",
      "Early stopping at epoch 12\n",
      "Evaluation for User11:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97       417\n",
      "         1.0       0.98      0.67      0.80        73\n",
      "\n",
      "    accuracy                           0.95       490\n",
      "   macro avg       0.96      0.83      0.88       490\n",
      "weighted avg       0.95      0.95      0.94       490\n",
      "\n",
      "AUC-ROC: 0.973\n",
      "\n",
      "Training User12 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User12...\n",
      "Teacher Evaluation for User12:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.71      0.83       440\n",
      "         1.0       0.28      1.00      0.44        50\n",
      "\n",
      "    accuracy                           0.74       490\n",
      "   macro avg       0.64      0.85      0.63       490\n",
      "weighted avg       0.93      0.74      0.79       490\n",
      "\n",
      "AUC-ROC (Teacher): 1.000\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User12...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6391 | Val Loss: 0.4069\n",
      "Epoch 2/50 - Train Loss: 0.4060 | Val Loss: 0.2160\n",
      "Epoch 3/50 - Train Loss: 0.3256 | Val Loss: 0.1788\n",
      "Epoch 4/50 - Train Loss: 0.3016 | Val Loss: 0.1802\n",
      "Epoch 5/50 - Train Loss: 0.2926 | Val Loss: 0.1880\n",
      "Epoch 6/50 - Train Loss: 0.2827 | Val Loss: 0.1995\n",
      "Epoch 7/50 - Train Loss: 0.2712 | Val Loss: 0.1913\n",
      "Epoch 8/50 - Train Loss: 0.2713 | Val Loss: 0.1987\n",
      "Early stopping at epoch 8\n",
      "Evaluation for User12:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.87      0.93       440\n",
      "         1.0       0.47      1.00      0.64        50\n",
      "\n",
      "    accuracy                           0.89       490\n",
      "   macro avg       0.74      0.94      0.79       490\n",
      "weighted avg       0.95      0.89      0.90       490\n",
      "\n",
      "AUC-ROC: 1.000\n",
      "\n",
      "Training User13 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User13...\n",
      "Teacher Evaluation for User13:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.79      0.88       443\n",
      "         1.0       0.33      1.00      0.50        47\n",
      "\n",
      "    accuracy                           0.81       490\n",
      "   macro avg       0.67      0.89      0.69       490\n",
      "weighted avg       0.94      0.81      0.84       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.999\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User13...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6610 | Val Loss: 0.4342\n",
      "Epoch 2/50 - Train Loss: 0.4317 | Val Loss: 0.2264\n",
      "Epoch 3/50 - Train Loss: 0.3454 | Val Loss: 0.1886\n",
      "Epoch 4/50 - Train Loss: 0.3131 | Val Loss: 0.1765\n",
      "Epoch 5/50 - Train Loss: 0.2981 | Val Loss: 0.1849\n",
      "Epoch 6/50 - Train Loss: 0.2874 | Val Loss: 0.1955\n",
      "Epoch 7/50 - Train Loss: 0.2847 | Val Loss: 0.1860\n",
      "Epoch 8/50 - Train Loss: 0.2786 | Val Loss: 0.1906\n",
      "Epoch 9/50 - Train Loss: 0.2740 | Val Loss: 0.1956\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User13:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97       443\n",
      "         1.0       0.68      0.98      0.80        47\n",
      "\n",
      "    accuracy                           0.95       490\n",
      "   macro avg       0.84      0.96      0.89       490\n",
      "weighted avg       0.97      0.95      0.96       490\n",
      "\n",
      "AUC-ROC: 0.996\n",
      "\n",
      "Training User14 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User14...\n",
      "Teacher Evaluation for User14:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.73      0.85       442\n",
      "         1.0       0.29      1.00      0.45        48\n",
      "\n",
      "    accuracy                           0.76       490\n",
      "   macro avg       0.64      0.87      0.65       490\n",
      "weighted avg       0.93      0.76      0.81       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.990\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User14...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6261 | Val Loss: 0.3892\n",
      "Epoch 2/50 - Train Loss: 0.4154 | Val Loss: 0.2261\n",
      "Epoch 3/50 - Train Loss: 0.3521 | Val Loss: 0.2041\n",
      "Epoch 4/50 - Train Loss: 0.3273 | Val Loss: 0.1848\n",
      "Epoch 5/50 - Train Loss: 0.3112 | Val Loss: 0.1763\n",
      "Epoch 6/50 - Train Loss: 0.3002 | Val Loss: 0.1841\n",
      "Epoch 7/50 - Train Loss: 0.2929 | Val Loss: 0.1829\n",
      "Epoch 8/50 - Train Loss: 0.2893 | Val Loss: 0.1936\n",
      "Epoch 9/50 - Train Loss: 0.2822 | Val Loss: 0.1918\n",
      "Epoch 10/50 - Train Loss: 0.2769 | Val Loss: 0.1842\n",
      "Early stopping at epoch 10\n",
      "Evaluation for User14:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.90      0.94       442\n",
      "         1.0       0.50      0.96      0.66        48\n",
      "\n",
      "    accuracy                           0.90       490\n",
      "   macro avg       0.75      0.93      0.80       490\n",
      "weighted avg       0.95      0.90      0.91       490\n",
      "\n",
      "AUC-ROC: 0.987\n",
      "\n",
      "Training User15 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User15...\n",
      "Teacher Evaluation for User15:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.79      0.89       422\n",
      "         1.0       0.44      1.00      0.61        68\n",
      "\n",
      "    accuracy                           0.82       490\n",
      "   macro avg       0.72      0.90      0.75       490\n",
      "weighted avg       0.92      0.82      0.85       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.988\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User15...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6293 | Val Loss: 0.3818\n",
      "Epoch 2/50 - Train Loss: 0.4026 | Val Loss: 0.2117\n",
      "Epoch 3/50 - Train Loss: 0.3350 | Val Loss: 0.1844\n",
      "Epoch 4/50 - Train Loss: 0.3130 | Val Loss: 0.1782\n",
      "Epoch 5/50 - Train Loss: 0.2994 | Val Loss: 0.1803\n",
      "Epoch 6/50 - Train Loss: 0.2849 | Val Loss: 0.1764\n",
      "Epoch 7/50 - Train Loss: 0.2758 | Val Loss: 0.1897\n",
      "Epoch 8/50 - Train Loss: 0.2804 | Val Loss: 0.1892\n",
      "Epoch 9/50 - Train Loss: 0.2731 | Val Loss: 0.1972\n",
      "Epoch 10/50 - Train Loss: 0.2736 | Val Loss: 0.1908\n",
      "Epoch 11/50 - Train Loss: 0.2705 | Val Loss: 0.1830\n",
      "Early stopping at epoch 11\n",
      "Evaluation for User15:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98       422\n",
      "         1.0       1.00      0.75      0.86        68\n",
      "\n",
      "    accuracy                           0.97       490\n",
      "   macro avg       0.98      0.88      0.92       490\n",
      "weighted avg       0.97      0.97      0.96       490\n",
      "\n",
      "AUC-ROC: 0.997\n",
      "\n",
      "Training User16 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User16...\n",
      "Teacher Evaluation for User16:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.82      0.90       430\n",
      "         1.0       0.43      0.98      0.60        60\n",
      "\n",
      "    accuracy                           0.84       490\n",
      "   macro avg       0.71      0.90      0.75       490\n",
      "weighted avg       0.93      0.84      0.86       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.983\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User16...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6658 | Val Loss: 0.4307\n",
      "Epoch 2/50 - Train Loss: 0.4339 | Val Loss: 0.2267\n",
      "Epoch 3/50 - Train Loss: 0.3481 | Val Loss: 0.1890\n",
      "Epoch 4/50 - Train Loss: 0.3184 | Val Loss: 0.1870\n",
      "Epoch 5/50 - Train Loss: 0.2970 | Val Loss: 0.1869\n",
      "Epoch 6/50 - Train Loss: 0.2965 | Val Loss: 0.1871\n",
      "Epoch 7/50 - Train Loss: 0.2877 | Val Loss: 0.1885\n",
      "Epoch 8/50 - Train Loss: 0.2833 | Val Loss: 0.1870\n",
      "Epoch 9/50 - Train Loss: 0.2741 | Val Loss: 0.2071\n",
      "Epoch 10/50 - Train Loss: 0.2724 | Val Loss: 0.1981\n",
      "Early stopping at epoch 10\n",
      "Evaluation for User16:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.95       430\n",
      "         1.0       0.61      0.98      0.75        60\n",
      "\n",
      "    accuracy                           0.92       490\n",
      "   macro avg       0.80      0.95      0.85       490\n",
      "weighted avg       0.95      0.92      0.93       490\n",
      "\n",
      "AUC-ROC: 0.980\n",
      "\n",
      "Training User17 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User17...\n",
      "Teacher Evaluation for User17:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.76      0.86       427\n",
      "         1.0       0.38      0.98      0.55        63\n",
      "\n",
      "    accuracy                           0.79       490\n",
      "   macro avg       0.69      0.87      0.71       490\n",
      "weighted avg       0.92      0.79      0.82       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.983\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User17...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6478 | Val Loss: 0.4140\n",
      "Epoch 2/50 - Train Loss: 0.4385 | Val Loss: 0.2416\n",
      "Epoch 3/50 - Train Loss: 0.3684 | Val Loss: 0.2058\n",
      "Epoch 4/50 - Train Loss: 0.3288 | Val Loss: 0.1884\n",
      "Epoch 5/50 - Train Loss: 0.3202 | Val Loss: 0.1832\n",
      "Epoch 6/50 - Train Loss: 0.3077 | Val Loss: 0.1809\n",
      "Epoch 7/50 - Train Loss: 0.3000 | Val Loss: 0.1813\n",
      "Epoch 8/50 - Train Loss: 0.2983 | Val Loss: 0.1738\n",
      "Epoch 9/50 - Train Loss: 0.2908 | Val Loss: 0.1784\n",
      "Epoch 10/50 - Train Loss: 0.2830 | Val Loss: 0.1863\n",
      "Epoch 11/50 - Train Loss: 0.2796 | Val Loss: 0.1872\n",
      "Epoch 12/50 - Train Loss: 0.2783 | Val Loss: 0.1752\n",
      "Epoch 13/50 - Train Loss: 0.2770 | Val Loss: 0.1989\n",
      "Early stopping at epoch 13\n",
      "Evaluation for User17:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.92      0.95       427\n",
      "         1.0       0.63      0.98      0.77        63\n",
      "\n",
      "    accuracy                           0.92       490\n",
      "   macro avg       0.82      0.95      0.86       490\n",
      "weighted avg       0.95      0.92      0.93       490\n",
      "\n",
      "AUC-ROC: 0.980\n",
      "\n",
      "Training User18 on: ['User4', 'User5', 'User26', 'User29']\n",
      "Train samples for teacher: 1960, Test samples: 490\n",
      "\n",
      "Training teacher model for User18...\n",
      "Teacher Evaluation for User18:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.79      0.88       450\n",
      "         1.0       0.30      1.00      0.46        40\n",
      "\n",
      "    accuracy                           0.81       490\n",
      "   macro avg       0.65      0.89      0.67       490\n",
      "weighted avg       0.94      0.81      0.85       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.983\n",
      "Train samples for student: 1960, Test samples: 490\n",
      "Training student model with distillation for User18...\n",
      "Teacher Train: 1568 | Teacher Val: 392 | Student Train: 1568 | Student Val: 392\n",
      "Epoch 1/50 - Train Loss: 0.7853 | Val Loss: 0.7151\n",
      "Epoch 2/50 - Train Loss: 0.7256 | Val Loss: 0.6456\n",
      "Epoch 3/50 - Train Loss: 0.6660 | Val Loss: 0.5682\n",
      "Epoch 4/50 - Train Loss: 0.6056 | Val Loss: 0.4817\n",
      "Epoch 5/50 - Train Loss: 0.5445 | Val Loss: 0.4032\n",
      "Epoch 6/50 - Train Loss: 0.4852 | Val Loss: 0.3343\n",
      "Epoch 7/50 - Train Loss: 0.4332 | Val Loss: 0.2790\n",
      "Epoch 8/50 - Train Loss: 0.4026 | Val Loss: 0.2549\n",
      "Epoch 9/50 - Train Loss: 0.3605 | Val Loss: 0.2220\n",
      "Epoch 10/50 - Train Loss: 0.3323 | Val Loss: 0.2129\n",
      "Epoch 11/50 - Train Loss: 0.2999 | Val Loss: 0.2051\n",
      "Epoch 12/50 - Train Loss: 0.2813 | Val Loss: 0.1957\n",
      "Epoch 13/50 - Train Loss: 0.2689 | Val Loss: 0.1906\n",
      "Epoch 14/50 - Train Loss: 0.2499 | Val Loss: 0.1939\n",
      "Epoch 15/50 - Train Loss: 0.2430 | Val Loss: 0.1882\n",
      "Epoch 16/50 - Train Loss: 0.2360 | Val Loss: 0.2082\n",
      "Epoch 17/50 - Train Loss: 0.2329 | Val Loss: 0.1978\n",
      "Epoch 18/50 - Train Loss: 0.2120 | Val Loss: 0.2075\n",
      "Epoch 19/50 - Train Loss: 0.2130 | Val Loss: 0.2222\n",
      "Epoch 20/50 - Train Loss: 0.2127 | Val Loss: 0.2073\n",
      "Early stopping at epoch 20\n",
      "Evaluation for User18:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.93       450\n",
      "         1.0       0.39      0.97      0.56        40\n",
      "\n",
      "    accuracy                           0.87       490\n",
      "   macro avg       0.69      0.92      0.74       490\n",
      "weighted avg       0.95      0.87      0.90       490\n",
      "\n",
      "AUC-ROC: 0.982\n",
      "\n",
      "Training User19 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User19...\n",
      "Teacher Evaluation for User19:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80       446\n",
      "         1.0       0.23      1.00      0.38        44\n",
      "\n",
      "    accuracy                           0.70       490\n",
      "   macro avg       0.62      0.84      0.59       490\n",
      "weighted avg       0.93      0.70      0.77       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.997\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User19...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6118 | Val Loss: 0.3641\n",
      "Epoch 2/50 - Train Loss: 0.3872 | Val Loss: 0.2134\n",
      "Epoch 3/50 - Train Loss: 0.3200 | Val Loss: 0.1833\n",
      "Epoch 4/50 - Train Loss: 0.3040 | Val Loss: 0.1832\n",
      "Epoch 5/50 - Train Loss: 0.2865 | Val Loss: 0.1817\n",
      "Epoch 6/50 - Train Loss: 0.2791 | Val Loss: 0.1923\n",
      "Epoch 7/50 - Train Loss: 0.2690 | Val Loss: 0.2055\n",
      "Epoch 8/50 - Train Loss: 0.2718 | Val Loss: 0.1958\n",
      "Epoch 9/50 - Train Loss: 0.2642 | Val Loss: 0.1960\n",
      "Epoch 10/50 - Train Loss: 0.2645 | Val Loss: 0.2073\n",
      "Early stopping at epoch 10\n",
      "Evaluation for User19:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94       446\n",
      "         1.0       0.47      1.00      0.64        44\n",
      "\n",
      "    accuracy                           0.90       490\n",
      "   macro avg       0.74      0.95      0.79       490\n",
      "weighted avg       0.95      0.90      0.91       490\n",
      "\n",
      "AUC-ROC: 0.995\n",
      "\n",
      "Training User20 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User20...\n",
      "Teacher Evaluation for User20:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.78      0.87       418\n",
      "         1.0       0.43      0.96      0.59        72\n",
      "\n",
      "    accuracy                           0.81       490\n",
      "   macro avg       0.71      0.87      0.73       490\n",
      "weighted avg       0.91      0.81      0.83       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.954\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User20...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6744 | Val Loss: 0.4366\n",
      "Epoch 2/50 - Train Loss: 0.4236 | Val Loss: 0.2220\n",
      "Epoch 3/50 - Train Loss: 0.3320 | Val Loss: 0.1803\n",
      "Epoch 4/50 - Train Loss: 0.3060 | Val Loss: 0.1806\n",
      "Epoch 5/50 - Train Loss: 0.2950 | Val Loss: 0.1881\n",
      "Epoch 6/50 - Train Loss: 0.2808 | Val Loss: 0.1810\n",
      "Epoch 7/50 - Train Loss: 0.2734 | Val Loss: 0.1904\n",
      "Epoch 8/50 - Train Loss: 0.2782 | Val Loss: 0.1995\n",
      "Early stopping at epoch 8\n",
      "Evaluation for User20:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97       418\n",
      "         1.0       0.84      0.81      0.82        72\n",
      "\n",
      "    accuracy                           0.95       490\n",
      "   macro avg       0.90      0.89      0.90       490\n",
      "weighted avg       0.95      0.95      0.95       490\n",
      "\n",
      "AUC-ROC: 0.961\n",
      "\n",
      "Training User21 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User21...\n",
      "Teacher Evaluation for User21:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.86      0.89       378\n",
      "         1.0       0.61      0.74      0.67       112\n",
      "\n",
      "    accuracy                           0.83       490\n",
      "   macro avg       0.76      0.80      0.78       490\n",
      "weighted avg       0.85      0.83      0.84       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.889\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User21...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6063 | Val Loss: 0.3536\n",
      "Epoch 2/50 - Train Loss: 0.3780 | Val Loss: 0.1913\n",
      "Epoch 3/50 - Train Loss: 0.3100 | Val Loss: 0.1603\n",
      "Epoch 4/50 - Train Loss: 0.2944 | Val Loss: 0.1521\n",
      "Epoch 5/50 - Train Loss: 0.2726 | Val Loss: 0.1565\n",
      "Epoch 6/50 - Train Loss: 0.2665 | Val Loss: 0.1575\n",
      "Epoch 7/50 - Train Loss: 0.2607 | Val Loss: 0.1575\n",
      "Epoch 8/50 - Train Loss: 0.2532 | Val Loss: 0.1630\n",
      "Epoch 9/50 - Train Loss: 0.2562 | Val Loss: 0.1677\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User21:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.97      0.88       378\n",
      "         1.0       0.64      0.16      0.26       112\n",
      "\n",
      "    accuracy                           0.79       490\n",
      "   macro avg       0.72      0.57      0.57       490\n",
      "weighted avg       0.76      0.79      0.73       490\n",
      "\n",
      "AUC-ROC: 0.877\n",
      "\n",
      "Training User22 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User23', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User22...\n",
      "Teacher Evaluation for User22:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.87      0.87       342\n",
      "         1.0       0.70      0.72      0.71       148\n",
      "\n",
      "    accuracy                           0.82       490\n",
      "   macro avg       0.79      0.79      0.79       490\n",
      "weighted avg       0.82      0.82      0.82       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.904\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User22...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6242 | Val Loss: 0.3601\n",
      "Epoch 2/50 - Train Loss: 0.3761 | Val Loss: 0.2086\n",
      "Epoch 3/50 - Train Loss: 0.3111 | Val Loss: 0.1924\n",
      "Epoch 4/50 - Train Loss: 0.2924 | Val Loss: 0.1792\n",
      "Epoch 5/50 - Train Loss: 0.2848 | Val Loss: 0.1630\n",
      "Epoch 6/50 - Train Loss: 0.2709 | Val Loss: 0.1721\n",
      "Epoch 7/50 - Train Loss: 0.2615 | Val Loss: 0.1713\n",
      "Epoch 8/50 - Train Loss: 0.2597 | Val Loss: 0.1803\n",
      "Epoch 9/50 - Train Loss: 0.2558 | Val Loss: 0.1797\n",
      "Epoch 10/50 - Train Loss: 0.2487 | Val Loss: 0.1836\n",
      "Early stopping at epoch 10\n",
      "Evaluation for User22:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.97      0.86       342\n",
      "         1.0       0.83      0.36      0.51       148\n",
      "\n",
      "    accuracy                           0.79       490\n",
      "   macro avg       0.80      0.67      0.69       490\n",
      "weighted avg       0.79      0.79      0.76       490\n",
      "\n",
      "AUC-ROC: 0.906\n",
      "\n",
      "Training User23 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User24', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User23...\n",
      "Teacher Evaluation for User23:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.75      0.85       432\n",
      "         1.0       0.34      0.97      0.50        58\n",
      "\n",
      "    accuracy                           0.77       490\n",
      "   macro avg       0.67      0.86      0.68       490\n",
      "weighted avg       0.92      0.77      0.81       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.967\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User23...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6116 | Val Loss: 0.3667\n",
      "Epoch 2/50 - Train Loss: 0.3872 | Val Loss: 0.2214\n",
      "Epoch 3/50 - Train Loss: 0.3257 | Val Loss: 0.1897\n",
      "Epoch 4/50 - Train Loss: 0.3036 | Val Loss: 0.1775\n",
      "Epoch 5/50 - Train Loss: 0.2846 | Val Loss: 0.1775\n",
      "Epoch 6/50 - Train Loss: 0.2751 | Val Loss: 0.1790\n",
      "Epoch 7/50 - Train Loss: 0.2688 | Val Loss: 0.1825\n",
      "Epoch 8/50 - Train Loss: 0.2685 | Val Loss: 0.1883\n",
      "Epoch 9/50 - Train Loss: 0.2626 | Val Loss: 0.1888\n",
      "Epoch 10/50 - Train Loss: 0.2605 | Val Loss: 0.1907\n",
      "Early stopping at epoch 10\n",
      "Evaluation for User23:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96       432\n",
      "         1.0       0.64      0.90      0.75        58\n",
      "\n",
      "    accuracy                           0.93       490\n",
      "   macro avg       0.81      0.91      0.85       490\n",
      "weighted avg       0.94      0.93      0.93       490\n",
      "\n",
      "AUC-ROC: 0.953\n",
      "\n",
      "Training User24 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User25', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User24...\n",
      "Teacher Evaluation for User24:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.78      0.88       411\n",
      "         1.0       0.46      0.97      0.63        79\n",
      "\n",
      "    accuracy                           0.81       490\n",
      "   macro avg       0.73      0.88      0.75       490\n",
      "weighted avg       0.91      0.81      0.84       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.971\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User24...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6170 | Val Loss: 0.3652\n",
      "Epoch 2/50 - Train Loss: 0.3985 | Val Loss: 0.2305\n",
      "Epoch 3/50 - Train Loss: 0.3352 | Val Loss: 0.2038\n",
      "Epoch 4/50 - Train Loss: 0.3048 | Val Loss: 0.1858\n",
      "Epoch 5/50 - Train Loss: 0.2914 | Val Loss: 0.1991\n",
      "Epoch 6/50 - Train Loss: 0.2824 | Val Loss: 0.1989\n",
      "Epoch 7/50 - Train Loss: 0.2757 | Val Loss: 0.1921\n",
      "Epoch 8/50 - Train Loss: 0.2688 | Val Loss: 0.2105\n",
      "Epoch 9/50 - Train Loss: 0.2671 | Val Loss: 0.2022\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User24:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.89      0.94       411\n",
      "         1.0       0.62      0.96      0.75        79\n",
      "\n",
      "    accuracy                           0.90       490\n",
      "   macro avg       0.80      0.92      0.84       490\n",
      "weighted avg       0.93      0.90      0.91       490\n",
      "\n",
      "AUC-ROC: 0.964\n",
      "\n",
      "Training User25 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User27', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User25...\n",
      "Teacher Evaluation for User25:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.72      0.83       448\n",
      "         1.0       0.25      1.00      0.40        42\n",
      "\n",
      "    accuracy                           0.74       490\n",
      "   macro avg       0.62      0.86      0.62       490\n",
      "weighted avg       0.94      0.74      0.80       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.997\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User25...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6025 | Val Loss: 0.3419\n",
      "Epoch 2/50 - Train Loss: 0.3708 | Val Loss: 0.1794\n",
      "Epoch 3/50 - Train Loss: 0.3112 | Val Loss: 0.1699\n",
      "Epoch 4/50 - Train Loss: 0.2845 | Val Loss: 0.1813\n",
      "Epoch 5/50 - Train Loss: 0.2803 | Val Loss: 0.1812\n",
      "Epoch 6/50 - Train Loss: 0.2721 | Val Loss: 0.1988\n",
      "Epoch 7/50 - Train Loss: 0.2651 | Val Loss: 0.2011\n",
      "Epoch 8/50 - Train Loss: 0.2602 | Val Loss: 0.1924\n",
      "Early stopping at epoch 8\n",
      "Evaluation for User25:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.82      0.90       448\n",
      "         1.0       0.34      1.00      0.51        42\n",
      "\n",
      "    accuracy                           0.83       490\n",
      "   macro avg       0.67      0.91      0.70       490\n",
      "weighted avg       0.94      0.83      0.87       490\n",
      "\n",
      "AUC-ROC: 0.997\n",
      "\n",
      "Training User26 on: ['User4', 'User5', 'User18', 'User29']\n",
      "Train samples for teacher: 1960, Test samples: 490\n",
      "\n",
      "Training teacher model for User26...\n",
      "Teacher Evaluation for User26:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.83      0.91       420\n",
      "         1.0       0.49      1.00      0.66        70\n",
      "\n",
      "    accuracy                           0.85       490\n",
      "   macro avg       0.75      0.91      0.78       490\n",
      "weighted avg       0.93      0.85      0.87       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.976\n",
      "Train samples for student: 1960, Test samples: 490\n",
      "Training student model with distillation for User26...\n",
      "Teacher Train: 1568 | Teacher Val: 392 | Student Train: 1568 | Student Val: 392\n",
      "Epoch 1/50 - Train Loss: 0.7502 | Val Loss: 0.6935\n",
      "Epoch 2/50 - Train Loss: 0.6933 | Val Loss: 0.6241\n",
      "Epoch 3/50 - Train Loss: 0.6231 | Val Loss: 0.5358\n",
      "Epoch 4/50 - Train Loss: 0.5507 | Val Loss: 0.4402\n",
      "Epoch 5/50 - Train Loss: 0.4792 | Val Loss: 0.3524\n",
      "Epoch 6/50 - Train Loss: 0.4114 | Val Loss: 0.2962\n",
      "Epoch 7/50 - Train Loss: 0.3679 | Val Loss: 0.2633\n",
      "Epoch 8/50 - Train Loss: 0.3281 | Val Loss: 0.2407\n",
      "Epoch 9/50 - Train Loss: 0.3041 | Val Loss: 0.2429\n",
      "Epoch 10/50 - Train Loss: 0.2747 | Val Loss: 0.2434\n",
      "Epoch 11/50 - Train Loss: 0.2560 | Val Loss: 0.2580\n",
      "Epoch 12/50 - Train Loss: 0.2515 | Val Loss: 0.2505\n",
      "Epoch 13/50 - Train Loss: 0.2340 | Val Loss: 0.2701\n",
      "Early stopping at epoch 13\n",
      "Evaluation for User26:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       420\n",
      "         1.0       0.93      0.93      0.93        70\n",
      "\n",
      "    accuracy                           0.98       490\n",
      "   macro avg       0.96      0.96      0.96       490\n",
      "weighted avg       0.98      0.98      0.98       490\n",
      "\n",
      "AUC-ROC: 0.997\n",
      "\n",
      "Training User27 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User28', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User27...\n",
      "Teacher Evaluation for User27:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.76      0.86       434\n",
      "         1.0       0.35      1.00      0.52        56\n",
      "\n",
      "    accuracy                           0.79       490\n",
      "   macro avg       0.67      0.88      0.69       490\n",
      "weighted avg       0.93      0.79      0.82       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.994\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User27...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6080 | Val Loss: 0.3784\n",
      "Epoch 2/50 - Train Loss: 0.4011 | Val Loss: 0.2389\n",
      "Epoch 3/50 - Train Loss: 0.3370 | Val Loss: 0.2120\n",
      "Epoch 4/50 - Train Loss: 0.3103 | Val Loss: 0.2070\n",
      "Epoch 5/50 - Train Loss: 0.2967 | Val Loss: 0.2082\n",
      "Epoch 6/50 - Train Loss: 0.2867 | Val Loss: 0.2258\n",
      "Epoch 7/50 - Train Loss: 0.2822 | Val Loss: 0.2302\n",
      "Epoch 8/50 - Train Loss: 0.2779 | Val Loss: 0.2285\n",
      "Epoch 9/50 - Train Loss: 0.2697 | Val Loss: 0.2516\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User27:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.98       434\n",
      "         1.0       0.73      1.00      0.84        56\n",
      "\n",
      "    accuracy                           0.96       490\n",
      "   macro avg       0.86      0.98      0.91       490\n",
      "weighted avg       0.97      0.96      0.96       490\n",
      "\n",
      "AUC-ROC: 0.994\n",
      "\n",
      "Training User28 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User30']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User28...\n",
      "Teacher Evaluation for User28:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.79      0.88       414\n",
      "         1.0       0.47      0.99      0.63        76\n",
      "\n",
      "    accuracy                           0.82       490\n",
      "   macro avg       0.73      0.89      0.76       490\n",
      "weighted avg       0.91      0.82      0.84       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.981\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User28...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6263 | Val Loss: 0.3684\n",
      "Epoch 2/50 - Train Loss: 0.3950 | Val Loss: 0.2153\n",
      "Epoch 3/50 - Train Loss: 0.3208 | Val Loss: 0.2138\n",
      "Epoch 4/50 - Train Loss: 0.3069 | Val Loss: 0.2150\n",
      "Epoch 5/50 - Train Loss: 0.2905 | Val Loss: 0.2337\n",
      "Epoch 6/50 - Train Loss: 0.2865 | Val Loss: 0.2149\n",
      "Epoch 7/50 - Train Loss: 0.2818 | Val Loss: 0.2177\n",
      "Epoch 8/50 - Train Loss: 0.2737 | Val Loss: 0.2286\n",
      "Early stopping at epoch 8\n",
      "Evaluation for User28:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.95      0.97       414\n",
      "         1.0       0.78      0.93      0.85        76\n",
      "\n",
      "    accuracy                           0.95       490\n",
      "   macro avg       0.88      0.94      0.91       490\n",
      "weighted avg       0.96      0.95      0.95       490\n",
      "\n",
      "AUC-ROC: 0.979\n",
      "\n",
      "Training User29 on: ['User4', 'User5', 'User18', 'User26']\n",
      "Train samples for teacher: 1960, Test samples: 490\n",
      "\n",
      "Training teacher model for User29...\n",
      "Teacher Evaluation for User29:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.95      0.93       394\n",
      "         1.0       0.76      0.58      0.66        96\n",
      "\n",
      "    accuracy                           0.88       490\n",
      "   macro avg       0.83      0.77      0.79       490\n",
      "weighted avg       0.88      0.88      0.88       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.854\n",
      "Train samples for student: 1960, Test samples: 490\n",
      "Training student model with distillation for User29...\n",
      "Teacher Train: 1568 | Teacher Val: 392 | Student Train: 1568 | Student Val: 392\n",
      "Epoch 1/50 - Train Loss: 0.7493 | Val Loss: 0.7060\n",
      "Epoch 2/50 - Train Loss: 0.6789 | Val Loss: 0.6432\n",
      "Epoch 3/50 - Train Loss: 0.6142 | Val Loss: 0.5758\n",
      "Epoch 4/50 - Train Loss: 0.5460 | Val Loss: 0.4949\n",
      "Epoch 5/50 - Train Loss: 0.4748 | Val Loss: 0.4062\n",
      "Epoch 6/50 - Train Loss: 0.4159 | Val Loss: 0.3297\n",
      "Epoch 7/50 - Train Loss: 0.3615 | Val Loss: 0.2667\n",
      "Epoch 8/50 - Train Loss: 0.3212 | Val Loss: 0.2241\n",
      "Epoch 9/50 - Train Loss: 0.2968 | Val Loss: 0.2033\n",
      "Epoch 10/50 - Train Loss: 0.2706 | Val Loss: 0.1782\n",
      "Epoch 11/50 - Train Loss: 0.2521 | Val Loss: 0.1626\n",
      "Epoch 12/50 - Train Loss: 0.2396 | Val Loss: 0.1490\n",
      "Epoch 13/50 - Train Loss: 0.2299 | Val Loss: 0.1539\n",
      "Epoch 14/50 - Train Loss: 0.2239 | Val Loss: 0.1434\n",
      "Epoch 15/50 - Train Loss: 0.2175 | Val Loss: 0.1407\n",
      "Epoch 16/50 - Train Loss: 0.2060 | Val Loss: 0.1309\n",
      "Epoch 17/50 - Train Loss: 0.2085 | Val Loss: 0.1431\n",
      "Epoch 18/50 - Train Loss: 0.2086 | Val Loss: 0.1316\n",
      "Epoch 19/50 - Train Loss: 0.2063 | Val Loss: 0.1372\n",
      "Epoch 20/50 - Train Loss: 0.1927 | Val Loss: 0.1367\n",
      "Epoch 21/50 - Train Loss: 0.1968 | Val Loss: 0.1261\n",
      "Epoch 22/50 - Train Loss: 0.1986 | Val Loss: 0.1229\n",
      "Epoch 23/50 - Train Loss: 0.1848 | Val Loss: 0.1347\n",
      "Epoch 24/50 - Train Loss: 0.1875 | Val Loss: 0.1262\n",
      "Epoch 25/50 - Train Loss: 0.1843 | Val Loss: 0.1323\n",
      "Epoch 26/50 - Train Loss: 0.1832 | Val Loss: 0.1336\n",
      "Epoch 27/50 - Train Loss: 0.1809 | Val Loss: 0.1378\n",
      "Early stopping at epoch 27\n",
      "Evaluation for User29:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      1.00      0.95       394\n",
      "         1.0       0.98      0.57      0.72        96\n",
      "\n",
      "    accuracy                           0.91       490\n",
      "   macro avg       0.94      0.79      0.84       490\n",
      "weighted avg       0.92      0.91      0.91       490\n",
      "\n",
      "AUC-ROC: 0.967\n",
      "\n",
      "Training User30 on: ['User1', 'User2', 'User3', 'User6', 'User8', 'User9', 'User10', 'User11', 'User12', 'User13', 'User14', 'User15', 'User16', 'User17', 'User19', 'User20', 'User21', 'User22', 'User23', 'User24', 'User25', 'User27', 'User28']\n",
      "Train samples for teacher: 11270, Test samples: 490\n",
      "\n",
      "Training teacher model for User30...\n",
      "Teacher Evaluation for User30:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.78      0.87       399\n",
      "         1.0       0.50      0.97      0.66        91\n",
      "\n",
      "    accuracy                           0.81       490\n",
      "   macro avg       0.74      0.87      0.76       490\n",
      "weighted avg       0.90      0.81      0.83       490\n",
      "\n",
      "AUC-ROC (Teacher): 0.962\n",
      "Train samples for student: 11270, Test samples: 490\n",
      "Training student model with distillation for User30...\n",
      "Teacher Train: 9016 | Teacher Val: 2254 | Student Train: 9016 | Student Val: 2254\n",
      "Epoch 1/50 - Train Loss: 0.6244 | Val Loss: 0.3563\n",
      "Epoch 2/50 - Train Loss: 0.4149 | Val Loss: 0.2070\n",
      "Epoch 3/50 - Train Loss: 0.3437 | Val Loss: 0.2025\n",
      "Epoch 4/50 - Train Loss: 0.3238 | Val Loss: 0.1877\n",
      "Epoch 5/50 - Train Loss: 0.3078 | Val Loss: 0.2044\n",
      "Epoch 6/50 - Train Loss: 0.2962 | Val Loss: 0.2077\n",
      "Epoch 7/50 - Train Loss: 0.2889 | Val Loss: 0.2124\n",
      "Epoch 8/50 - Train Loss: 0.2846 | Val Loss: 0.1994\n",
      "Epoch 9/50 - Train Loss: 0.2872 | Val Loss: 0.2070\n",
      "Early stopping at epoch 9\n",
      "Evaluation for User30:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96       399\n",
      "         1.0       0.81      0.88      0.84        91\n",
      "\n",
      "    accuracy                           0.94       490\n",
      "   macro avg       0.89      0.92      0.90       490\n",
      "weighted avg       0.94      0.94      0.94       490\n",
      "\n",
      "AUC-ROC: 0.969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# def train_student_with_distillation(student, teacher, x_student, x_teacher, y_train, proj_lstm, proj_dense, epochs=50, batch_size=32):\n",
    "#     optimizer = Adam()\n",
    "#     bce_loss = BinaryCrossentropy()\n",
    "#     mse_loss = MeanSquaredError()\n",
    "\n",
    "#     # Combine inputs into a dataset: ((student_input, teacher_input), labels)\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(((x_student, x_teacher), y_train))\n",
    "#     dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         total_hard_loss = 0.0\n",
    "#         total_feat_loss = 0.0\n",
    "#         total_batches = 0\n",
    "\n",
    "#         for (x_batch_student, x_batch_teacher), y_batch in dataset:\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 # Teacher output (no gradients)\n",
    "#                 t_lstm, t_dense, t_output = teacher(x_batch_teacher, training=False)\n",
    "\n",
    "#                 # Student output (with gradients)\n",
    "#                 s_lstm, s_dense, s_output = student(x_batch_student, training=True)\n",
    "\n",
    "#                 # Hard label loss (true label)\n",
    "#                 hard_loss = bce_loss(y_batch, s_output)\n",
    "\n",
    "#                 #  Project student outputs to match teacher dimensions\n",
    "#                 s_lstm_proj = proj_lstm(s_lstm)\n",
    "#                 s_dense_proj = proj_dense(s_dense)\n",
    "\n",
    "#                 # Feature distillation loss\n",
    "#                 feat_loss = mse_loss(t_lstm, s_lstm_proj) + mse_loss(t_dense, s_dense_proj)\n",
    "\n",
    "#                 # Total loss (weighted sum)\n",
    "#                 total_loss = hard_loss + 0.5 * feat_loss\n",
    "\n",
    "#             # Backpropagation\n",
    "#             grads = tape.gradient(total_loss, student.trainable_weights + proj_lstm.trainable_weights + proj_dense.trainable_weights)\n",
    "#             optimizer.apply_gradients(zip(grads, student.trainable_weights + proj_lstm.trainable_weights + proj_dense.trainable_weights))\n",
    "\n",
    "#             total_hard_loss += hard_loss.numpy()\n",
    "#             total_feat_loss += feat_loss.numpy()\n",
    "#             total_batches += 1\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} - Hard Loss: {total_hard_loss/total_batches:.4f} | Feature Loss: {total_feat_loss/total_batches:.4f}\")\n",
    "\n",
    "def train_student_with_distillation(\n",
    "    student, teacher, x_student, x_teacher, y_train,\n",
    "    proj_lstm, proj_dense,\n",
    "    epochs=50, batch_size=32,\n",
    "    patience=5, val_split=0.2\n",
    "):\n",
    "    optimizer = Adam()\n",
    "    bce_loss = BinaryCrossentropy()\n",
    "    mse_loss = MeanSquaredError()\n",
    "\n",
    "    # Split train/val manually\n",
    "    num_val = int(len(y_train) * val_split)\n",
    "    x_student_train, x_student_val = x_student[:-num_val], x_student[-num_val:]\n",
    "    x_teacher_train, x_teacher_val = x_teacher[:-num_val], x_teacher[-num_val:]\n",
    "    y_train_train, y_val = y_train[:-num_val], y_train[-num_val:]\n",
    "\n",
    "    print(f\"Teacher Train: {len(x_teacher_train)} | Teacher Val: {len(x_teacher_val)} | Student Train: {len(x_student_train)} | Student Val: {len(x_student_val)}\")\n",
    "    # Prepare datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((x_student_train, x_teacher_train), y_train_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((x_student_val, x_teacher_val), y_val))\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    wait = 0\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        total_hard_loss = 0.0\n",
    "        total_feat_loss = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        for (x_batch_student, x_batch_teacher), y_batch in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                t_lstm, t_dense, t_output = teacher(x_batch_teacher, training=False)\n",
    "                s_lstm, s_dense, s_output = student(x_batch_student, training=True)\n",
    "\n",
    "                hard_loss = bce_loss(y_batch, s_output)\n",
    "                s_lstm_proj = proj_lstm(s_lstm)\n",
    "                s_dense_proj = proj_dense(s_dense)\n",
    "                feat_loss = mse_loss(t_lstm, s_lstm_proj) + mse_loss(t_dense, s_dense_proj)\n",
    "\n",
    "                total_loss = hard_loss + 0.5 * feat_loss\n",
    "\n",
    "            grads = tape.gradient(total_loss, student.trainable_weights + proj_lstm.trainable_weights + proj_dense.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, student.trainable_weights + proj_lstm.trainable_weights + proj_dense.trainable_weights))\n",
    "\n",
    "            total_hard_loss += hard_loss.numpy()\n",
    "            total_feat_loss += feat_loss.numpy()\n",
    "            total_batches += 1\n",
    "\n",
    "        train_loss = (total_hard_loss + 0.5 * total_feat_loss) / total_batches\n",
    "\n",
    "        # Validation loop\n",
    "        val_losses = []\n",
    "        for (x_batch_student, x_batch_teacher), y_batch in val_dataset:\n",
    "            t_lstm, t_dense, t_output = teacher(x_batch_teacher, training=False)\n",
    "            s_lstm, s_dense, s_output = student(x_batch_student, training=False)\n",
    "\n",
    "            hard_loss = bce_loss(y_batch, s_output)\n",
    "            s_lstm_proj = proj_lstm(s_lstm)\n",
    "            s_dense_proj = proj_dense(s_dense)\n",
    "            feat_loss = mse_loss(t_lstm, s_lstm_proj) + mse_loss(t_dense, s_dense_proj)\n",
    "\n",
    "            val_losses.append(hard_loss.numpy() + 0.5 * feat_loss.numpy())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            wait = 0\n",
    "            # Save best weights\n",
    "            best_weights = (\n",
    "                student.get_weights(),\n",
    "                proj_lstm.get_weights(),\n",
    "                proj_dense.get_weights()\n",
    "            )\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                # Restore best weights\n",
    "                student.set_weights(best_weights[0])\n",
    "                proj_lstm.set_weights(best_weights[1])\n",
    "                proj_dense.set_weights(best_weights[2])\n",
    "                break\n",
    "\n",
    "\n",
    "def main():\n",
    "    # User profiles with predefined clusters\n",
    "    df_profiles = pd.DataFrame({\n",
    "        \"User\": [f\"User{i}\" for i in range(1,31) if i != 7],\n",
    "        \"Cluster\": [0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
    "                    0, 0, 0, 0, 0, 0, 0, 1,\n",
    "                    0, 0, 0, 0, 0, 0, 0, 1,\n",
    "                    0, 0, 1, 0]\n",
    "    })\n",
    "\n",
    "    user_inputs = {}\n",
    "    user_labels = {}\n",
    "\n",
    "    for i in range(1, 31):\n",
    "        if i == 7:  # Skip missing user\n",
    "            continue\n",
    "        user_id = f\"User{i}\"\n",
    "        try:\n",
    "            physio_file = f\"case_dataset-master/case_dataset-master/data/raw/physiological/sub{i}_DAQ.txt\"\n",
    "            df_physio = load_data(physio_file)\n",
    "            segmented_data = segment_data(df_physio)\n",
    "            \n",
    "            physio_features = segmented_data[['ECG_mean', 'BVP_mean', 'GSR_mean', 'Resp_mean', 'Skin_temp_mean', 'EMG_mean']].values\n",
    "            change_scores = compute_rulsif_change_scores(physio_features)\n",
    "            opportune_moments = label_opportune_moments(change_scores)\n",
    "            \n",
    "            labels = np.zeros(len(segmented_data))\n",
    "            labels[opportune_moments] = 1\n",
    "            \n",
    "            # Prepare inputs for teacher and student separately\n",
    "            input_teacher = prepare_input(segmented_data.copy(), change_scores)\n",
    "            input_student = prepare_input_student(segmented_data.copy(), change_scores)\n",
    "            \n",
    "            # Remove rows with NaNs (if any)\n",
    "            valid_indices_teacher = ~np.isnan(input_teacher).any(axis=1)\n",
    "            valid_indices_student = ~np.isnan(input_student).any(axis=1)\n",
    "            \n",
    "            # Keep intersection of valid indices for teacher and student inputs\n",
    "            valid_indices = valid_indices_teacher & valid_indices_student\n",
    "            \n",
    "            input_teacher = input_teacher[valid_indices]\n",
    "            input_student = input_student[valid_indices]\n",
    "            labels_cleaned = labels[valid_indices]\n",
    "            \n",
    "            user_inputs[user_id] = {\n",
    "                'teacher': input_teacher,\n",
    "                'student': input_student\n",
    "            }\n",
    "            user_labels[user_id] = labels_cleaned\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Data for {user_id} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    for user, input_data in user_inputs.items():\n",
    "        cluster = df_profiles[df_profiles[\"User\"] == user][\"Cluster\"].values[0]\n",
    "        similar_users = [u for u in df_profiles[df_profiles[\"Cluster\"] == cluster][\"User\"] if u != user]\n",
    "\n",
    "        # Preparing training data and labels for teacher and student separately\n",
    "        train_data_teacher, train_data_student, train_labels = [], [], []\n",
    "        for u in similar_users:\n",
    "            if u in user_inputs:\n",
    "                train_data_teacher.append(user_inputs[u]['teacher'])\n",
    "                train_data_student.append(user_inputs[u]['student'])\n",
    "                train_labels.append(user_labels[u])\n",
    "\n",
    "        if not train_data_teacher or not train_data_student:\n",
    "            print(f\"No training data for {user}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        proj_lstm = Dense(32)\n",
    "        proj_dense = Dense(16)\n",
    "\n",
    "        train_data_teacher = np.vstack(train_data_teacher)\n",
    "        train_data_student = np.vstack(train_data_student)\n",
    "        train_labels = np.hstack(train_labels)\n",
    "\n",
    "        # Test data\n",
    "        test_data_teacher = input_data['teacher']\n",
    "        test_data_student = input_data['student']\n",
    "        test_labels = user_labels[user]\n",
    "\n",
    "        # Reshaping inputs for LSTM: [samples, timesteps, features]\n",
    "        train_data_teacher = train_data_teacher.reshape((train_data_teacher.shape[0], 1, train_data_teacher.shape[1]))\n",
    "        train_data_student = train_data_student.reshape((train_data_student.shape[0], 1, train_data_student.shape[1]))\n",
    "        test_data_teacher = test_data_teacher.reshape((test_data_teacher.shape[0], 1, test_data_teacher.shape[1]))\n",
    "        test_data_student = test_data_student.reshape((test_data_student.shape[0], 1, test_data_student.shape[1]))\n",
    "\n",
    "        print(f\"\\nTraining {user} on: {[u for u in similar_users if u in user_inputs]}\")\n",
    "        print(f\"Train samples for teacher: {len(train_data_teacher)}, Test samples: {len(test_data_teacher)}\")\n",
    "\n",
    "        print(f\"\\nTraining teacher model for {user}...\")\n",
    "        teacher_model = build_p_lstm_for_distillation((train_data_teacher.shape[1], train_data_teacher.shape[2]))\n",
    "        teacher_model.compile(optimizer=Adam(0.001), loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=np.unique(train_labels),\n",
    "            y=train_labels\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        teacher_model.fit(\n",
    "            train_data_teacher, train_labels,\n",
    "            validation_split=0.2,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weights,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        t_lstm, t_dense, teacher_preds = teacher_model.predict(test_data_teacher)\n",
    "        teacher_pred_classes = (teacher_preds > 0.5).astype(int)\n",
    "\n",
    "        print(f\"Teacher Evaluation for {user}:\")\n",
    "        print(classification_report(test_labels, teacher_pred_classes))\n",
    "        print(f\"AUC-ROC (Teacher): {roc_auc_score(test_labels, teacher_preds):.3f}\")\n",
    "\n",
    "        print(f\"Train samples for student: {len(train_data_student)}, Test samples: {len(test_data_student)}\")\n",
    "\n",
    "        print(f\"Training student model with distillation for {user}...\")\n",
    "        student_model = build_student_lstm((train_data_student.shape[1], train_data_student.shape[2]))\n",
    "        train_student_with_distillation(\n",
    "            student_model, teacher_model,\n",
    "            train_data_student, train_data_teacher,\n",
    "            train_labels,\n",
    "            proj_lstm,\n",
    "            proj_dense,\n",
    "            epochs=50,\n",
    "            batch_size=32\n",
    "        )\n",
    "\n",
    "        # Evaluate student on test student data\n",
    "        s_lstm, s_dense, y_pred = student_model.predict(test_data_student)\n",
    "        y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        print(f\"Evaluation for {user}:\")\n",
    "        print(classification_report(test_labels, y_pred_classes))\n",
    "        print(f\"AUC-ROC: {roc_auc_score(test_labels, y_pred):.3f}\")\n",
    "\n",
    "\n",
    "# Then call main()\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6f978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YO",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
